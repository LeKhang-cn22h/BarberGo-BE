"""
Hair Style Generation Service - SD1.5 + Inpainting vá»›i MediaPipe
FAST VERSION: Cháº¡y nhanh, Ã­t VRAM, dá»… debug
"""

import torch
import cv2
import numpy as np
from PIL import Image
from typing import List, Dict, Tuple, Optional
import logging
import time
import mediapipe as mp
import os
from pathlib import Path

from diffusers import (
    StableDiffusionInpaintPipeline,
    UniPCMultistepScheduler,
    EulerAncestralDiscreteScheduler
)

logger = logging.getLogger(__name__)


class HairStyleConfig:
    """Config cho SD1.5 + Inpainting"""
    # Model SD1.5 Inpainting
    SD15_INPAINT_MODEL = "runwayml/stable-diffusion-inpainting"

    # Hoáº·c cÃ¡c model alternatives:
    # "stabilityai/stable-diffusion-2-inpainting"
    # "digiplay/AbsoluteReality_v1.8.1"  # Realistic
    # "SG161222/Realistic_Vision_V5.1_noVAE"  # Photorealistic

    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    USE_XFORMERS = True if DEVICE == "cuda" else False

    # SD1.5 dÃ¹ng 512x512
    IMAGE_SIZE = (512, 512)

    # Generation settings
    NUM_INFERENCE_STEPS = 30
    GUIDANCE_SCALE = 7.5
    DENOISING_STRENGTH = 0.75  # Cao vÃ¬ mask nhá»

    # Mask settings
    MASK_DILATE = 10
    FACE_PROTECTION_PADDING = 0.25

    # Debug
    DEBUG_MODE = True
    DEBUG_DIR = "./debug_masks"


class FaceDetectorMediaPipe:
    """Face detection vá»›i MediaPipe - Nhanh vÃ  chÃ­nh xÃ¡c"""

    def __init__(self, config: HairStyleConfig):
        self.config = config
        logger.info("Initializing MediaPipe Face Detection...")

        # Khá»Ÿi táº¡o MediaPipe
        mp_face_detection = mp.solutions.face_detection
        self.face_detection = mp_face_detection.FaceDetection(
            model_selection=1,  # 1=full-range detection
            min_detection_confidence=0.5
        )

        logger.info("âœ“ MediaPipe Face Detection initialized!")

    def get_face_info(self, image: np.ndarray) -> Optional[Dict]:
        """Get face bounding box"""
        try:
            # Convert BGR to RGB
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            h, w = image.shape[:2]

            # Detect face
            results = self.face_detection.process(rgb_image)

            if results.detections:
                # Láº¥y face cÃ³ confidence cao nháº¥t
                detection = results.detections[0]
                bbox = detection.location_data.relative_bounding_box

                # Convert relative to absolute coordinates
                x = int(bbox.xmin * w)
                y = int(bbox.ymin * h)
                width = int(bbox.width * w)
                height = int(bbox.height * h)

                bbox_abs = [
                    max(0, x),
                    max(0, y),
                    min(w, x + width),
                    min(h, y + height)
                ]

                return {
                    'bbox': bbox_abs,
                    'confidence': detection.score[0],
                    'width': width,
                    'height': height
                }

            return None

        except Exception as e:
            logger.error(f"MediaPipe detection error: {e}")
            return None


    def get_forehead_position(self, image: np.ndarray, face_info: Dict) -> int:
        """Láº¥y Ä‘Æ°á»ng chÃ¢n tÃ³c CHUáº¨N NHáº¤T báº±ng MediaPipe Face Mesh"""
        h, w = image.shape[:2]

        # Khá»Ÿi táº¡o MediaPipe Face Mesh náº¿u chÆ°a cÃ³
        if not hasattr(self, 'face_mesh'):
            import mediapipe as mp
            mp_face_mesh = mp.solutions.face_mesh
            self.face_mesh = mp_face_mesh.FaceMesh(
                static_image_mode=True,
                max_num_faces=1,
                refine_landmarks=True,
                min_detection_confidence=0.5
            )

        try:
            # Convert BGR to RGB
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # Process vá»›i Face Mesh
            results = self.face_mesh.process(rgb_image)

            if results.multi_face_landmarks:
                face_landmarks = results.multi_face_landmarks[0]

                # Láº¥y cÃ¡c landmarks quan trá»ng cho chÃ¢n tÃ³c
                # Landmark indices cho forehead/hairline:
                # 10: giá»¯a trÃ¡n trÃªn
                # 67, 69, 109: trÃ¡n trÃ¡i
                # 298, 299, 300: trÃ¡n pháº£i
                # 151: Ä‘iá»ƒm giá»¯a chÃ¢n tÃ³c

                forehead_points = []

                # Äiá»ƒm giá»¯a chÃ¢n tÃ³c (landmark 151)
                if len(face_landmarks.landmark) > 151:
                    lm = face_landmarks.landmark[151]
                    px = int(lm.x * w)
                    py = int(lm.y * h)
                    forehead_points.append((px, py))
                    print(f"ğŸ” Landmark 151 (mid forehead): y={py}")

                # Äiá»ƒm trÃªn trÃ¡n (landmark 10)
                if len(face_landmarks.landmark) > 10:
                    lm = face_landmarks.landmark[10]
                    px = int(lm.x * w)
                    py = int(lm.y * h)
                    forehead_points.append((px, py))
                    print(f"ğŸ” Landmark 10 (forehead top): y={py}")

                # Láº¥y Ä‘iá»ƒm tháº¥p nháº¥t (gáº§n lÃ´ng mÃ y nháº¥t)
                if forehead_points:
                    # Láº¥y tá»a Ä‘á»™ y cá»§a táº¥t cáº£ Ä‘iá»ƒm trÃ¡n
                    y_values = [p[1] for p in forehead_points]

                    # Chá»n Ä‘iá»ƒm THáº¤P NHáº¤T (gáº§n lÃ´ng mÃ y nháº¥t) lÃ m hairline
                    hairline_y = min(y_values)

                    # Äiá»u chá»‰nh: lÃ¹i lÃªn 5% face height Ä‘á»ƒ cháº¯c cháº¯n
                    x1, y1, x2, y2 = face_info['bbox']
                    face_height = y2 - y1
                    hairline_y = max(0, hairline_y - int(face_height * 0.05))

                    print(f"âœ… Calculated hairline_y: {hairline_y}")

                    # Váº½ debug
                    if self.config.DEBUG_MODE:
                        debug_img = image.copy()
                        # Váº½ landmarks
                        for px, py in forehead_points:
                            cv2.circle(debug_img, (px, py), 5, (0, 255, 0), -1)
                        # Váº½ hairline
                        cv2.line(debug_img, (0, hairline_y), (w, hairline_y), (0, 0, 255), 3)
                        cv2.imwrite(f'{self.config.DEBUG_DIR}/hairline_landmarks.png', debug_img)

                    return hairline_y

            # Fallback: dÃ¹ng logic cÅ© náº¿u khÃ´ng detect Ä‘Æ°á»£c landmarks
            print("âš ï¸ Cannot detect landmarks, using fallback method")
            x1, y1, x2, y2 = face_info['bbox']
            face_height = y2 - y1
            hairline_y = int(y1 + face_height * 0.33)
            return hairline_y

        except Exception as e:
            print(f"âŒ Error in hairline detection: {e}")
            # Fallback
            x1, y1, x2, y2 = face_info['bbox']
            face_height = y2 - y1
            return int(y1 + face_height * 0.33)

    def create_face_protection_mask(
            self,
            image: np.ndarray,
            face_info: Dict
    ) -> np.ndarray:
        """Táº¡o mask báº£o vá»‡ máº·t CHÃNH XÃC (phÃ­a DÆ¯á»šI hairline)"""
        h, w = image.shape[:2]

        # 1. Khá»Ÿi táº¡o mask TRáº®NG toÃ n bá»™ (thay Ä‘á»•i toÃ n bá»™)
        mask = np.ones((h, w), dtype=np.uint8) * 255

        # 2. Láº¥y hairline chÃ­nh xÃ¡c
        hairline_y = self.get_forehead_position(image, face_info)

        # 3. Láº¥y landmarks máº·t Ä‘á»ƒ táº¡o mask chÃ­nh xÃ¡c
        try:
            if hasattr(self, 'face_mesh'):
                rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                results = self.face_mesh.process(rgb_image)

                if results.multi_face_landmarks:
                    face_landmarks = results.multi_face_landmarks[0]

                    # Táº¡o mask máº·t tá»« landmarks
                    face_points = []

                    # Láº¥y cÃ¡c landmarks xung quanh máº·t (trá»« tÃ³c)
                    # Jawline, cheeks, chin, forehead (pháº§n dÆ°á»›i hairline)
                    jaw_indices = [152, 148, 176, 149, 150, 136, 172, 58, 132]
                    cheek_indices = [116, 117, 118, 119, 100, 47, 126, 209, 49]

                    for idx in jaw_indices + cheek_indices:
                        if len(face_landmarks.landmark) > idx:
                            lm = face_landmarks.landmark[idx]
                            px = int(lm.x * w)
                            py = int(lm.y * h)
                            if py > hairline_y:  # Chá»‰ láº¥y points dÆ°á»›i hairline
                                face_points.append((px, py))

                    if len(face_points) > 3:
                        # Táº¡o convex hull tá»« cÃ¡c points
                        points_array = np.array(face_points, dtype=np.int32)
                        hull = cv2.convexHull(points_array)

                        # Váº½ convex hull = ÄEN (0) trÃªn mask táº¡m
                        face_region = np.zeros((h, w), dtype=np.uint8)
                        cv2.fillConvexPoly(face_region, hull, 255)

                        # Trá»« vÃ¹ng máº·t ra khá»i mask tá»•ng
                        mask = cv2.subtract(mask, face_region)

                        print(f"âœ… Created precise face mask from {len(face_points)} landmarks")

                        # Debug
                        if self.config.DEBUG_MODE:
                            cv2.imwrite(f'{self.config.DEBUG_DIR}/face_landmarks_mask.png', mask)

                        return mask
        except Exception as e:
            print(f"âš ï¸ Landmark-based face mask failed: {e}")

        # 4. Fallback: dÃ¹ng phÆ°Æ¡ng phÃ¡p cÅ©
        print("âš ï¸ Using fallback face protection method")

        x1, y1, x2, y2 = face_info['bbox']
        face_width = x2 - x1
        face_height = y2 - y1

        # Táº¡o mask táº¡m cho vÃ¹ng máº·t
        face_region = np.zeros((h, w), dtype=np.uint8)

        # Váº½ hÃ¬nh oval bao quanh máº·t (tá»« hairline_y xuá»‘ng)
        center_x = (x1 + x2) // 2
        center_y = (hairline_y + y2) // 2
        axes_x = int(face_width * 0.6)
        axes_y = int((y2 - hairline_y) * 0.7)

        cv2.ellipse(
            face_region,
            (center_x, center_y),
            (axes_x, axes_y),
            0, 0, 360,
            255,
            -1
        )

        # Trá»« vÃ¹ng máº·t
        mask = cv2.subtract(mask, face_region)

        # LÃ m má»m
        mask = cv2.GaussianBlur(mask, (31, 31), 15)
        _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)

        return mask
# class FaceDetectorMediaPipe:
#     """Face detection vá»›i MediaPipe - Nhanh vÃ  chÃ­nh xÃ¡c"""
#
#     def __init__(self, config: HairStyleConfig):
#         self.config = config
#         logger.info("Initializing MediaPipe Face Detection...")
#
#         # Khá»Ÿi táº¡o MediaPipe
#         mp_face_detection = mp.solutions.face_detection
#         self.face_detection = mp_face_detection.FaceDetection(
#             model_selection=1,  # 1=full-range detection
#             min_detection_confidence=0.5
#         )
#
#         logger.info("âœ“ MediaPipe Face Detection initialized!")
#
#     def get_face_info(self, image: np.ndarray) -> Optional[Dict]:
#         """Get face bounding box"""
#         try:
#             # Convert BGR to RGB
#             rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
#             h, w = image.shape[:2]
#
#             # Detect face
#             results = self.face_detection.process(rgb_image)
#
#             if results.detections:
#                 # Láº¥y face cÃ³ confidence cao nháº¥t
#                 detection = results.detections[0]
#                 bbox = detection.location_data.relative_bounding_box
#
#                 # Convert relative to absolute coordinates
#                 x = int(bbox.xmin * w)
#                 y = int(bbox.ymin * h)
#                 width = int(bbox.width * w)
#                 height = int(bbox.height * h)
#
#                 bbox_abs = [
#                     max(0, x),
#                     max(0, y),
#                     min(w, x + width),
#                     min(h, y + height)
#                 ]
#
#                 return {
#                     'bbox': bbox_abs,
#                     'confidence': detection.score[0],
#                     'width': width,
#                     'height': height
#                 }
#
#             return None
#
#         except Exception as e:
#             logger.error(f"MediaPipe detection error: {e}")
#             return None
#
#     def get_forehead_position(self, image: np.ndarray, face_info: Dict) -> int:
#         """Láº¥y vá»‹ trÃ­ CHÃ‚N MÃ€Y (thay vÃ¬ chÃ¢n tÃ³c)"""
#         h, w = image.shape[:2]
#
#         # Khá»Ÿi táº¡o MediaPipe Face Mesh náº¿u chÆ°a cÃ³
#         if not hasattr(self, 'face_mesh'):
#             import mediapipe as mp
#             mp_face_mesh = mp.solutions.face_mesh
#             self.face_mesh = mp_face_mesh.FaceMesh(
#                 static_image_mode=True,
#                 max_num_faces=1,
#                 refine_landmarks=True,
#                 min_detection_confidence=0.5
#             )
#
#         try:
#             # Convert BGR to RGB
#             rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
#
#             # Process vá»›i Face Mesh
#             results = self.face_mesh.process(rgb_image)
#
#             if results.multi_face_landmarks:
#                 face_landmarks = results.multi_face_landmarks[0]
#
#                 # Láº¥y cÃ¡c landmarks cho CHÃ‚N MÃ€Y (khÃ´ng pháº£i chÃ¢n tÃ³c)
#                 # Landmark indices cho chÃ¢n mÃ y:
#                 # ChÃ¢n mÃ y pháº£i: 55, 65, 52, 53, 46 (pháº§n dÆ°á»›i)
#                 # ChÃ¢n mÃ y trÃ¡i: 285, 295, 282, 283, 276 (pháº§n dÆ°á»›i)
#
#                 eyebrow_points = []
#
#                 # ChÃ¢n mÃ y pháº£i - pháº§n dÆ°á»›i cÃ¹ng
#                 right_eyebrow_indices = [55, 65, 52, 53, 46]
#                 for idx in right_eyebrow_indices:
#                     if len(face_landmarks.landmark) > idx:
#                         lm = face_landmarks.landmark[idx]
#                         px = int(lm.x * w)
#                         py = int(lm.y * h)
#                         eyebrow_points.append((px, py))
#                         print(f"ğŸ” Right eyebrow landmark {idx}: y={py}")
#
#                 # ChÃ¢n mÃ y trÃ¡i - pháº§n dÆ°á»›i cÃ¹ng
#                 left_eyebrow_indices = [285, 295, 282, 283, 276]
#                 for idx in left_eyebrow_indices:
#                     if len(face_landmarks.landmark) > idx:
#                         lm = face_landmarks.landmark[idx]
#                         px = int(lm.x * w)
#                         py = int(lm.y * h)
#                         eyebrow_points.append((px, py))
#                         print(f"ğŸ” Left eyebrow landmark {idx}: y={py}")
#
#                 if eyebrow_points:
#                     # Láº¥y tá»a Ä‘á»™ y cá»§a táº¥t cáº£ Ä‘iá»ƒm chÃ¢n mÃ y
#                     y_values = [p[1] for p in eyebrow_points]
#
#                     # Chá»n Ä‘iá»ƒm CAO NHáº¤T (y nhá» nháº¥t) lÃ m Ä‘Æ°á»ng chÃ¢n mÃ y
#                     # NhÆ°ng thá»±c táº¿ chÃºng ta muá»‘n láº¥y pháº§n DÆ¯á»šI CÃ™NG cá»§a chÃ¢n mÃ y (y lá»›n nháº¥t)
#                     eyebrow_bottom_y = max(y_values)
#
#                     # ThÃªm margin an toÃ n: xuá»‘ng thÃªm 5% face height Ä‘á»ƒ cháº¯c cháº¯n báº£o vá»‡ toÃ n bá»™ máº·t
#                     x1, y1, x2, y2 = face_info['bbox']
#                     face_height = y2 - y1
#                     eyebrow_line_y = min(h - 1, eyebrow_bottom_y + int(face_height * 0.05))
#
#                     print(f"âœ… Calculated eyebrow_line_y: {eyebrow_line_y} (from {len(eyebrow_points)} eyebrow points)")
#
#                     # Váº½ debug
#                     if self.config.DEBUG_MODE:
#                         debug_img = image.copy()
#                         # Váº½ landmarks chÃ¢n mÃ y
#                         for px, py in eyebrow_points:
#                             cv2.circle(debug_img, (px, py), 3, (0, 255, 0), -1)
#                         # Váº½ Ä‘Æ°á»ng chÃ¢n mÃ y
#                         cv2.line(debug_img, (0, eyebrow_line_y), (w, eyebrow_line_y), (0, 0, 255), 3)
#                         cv2.putText(debug_img, f'Eyebrow Line: y={eyebrow_line_y}',
#                                     (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7,
#                                     (0, 0, 255), 2)
#                         cv2.imwrite(f'{self.config.DEBUG_DIR}/eyebrow_po7sition.png', debug_img)
#
#                     return eyebrow_line_y
#
#             # Fallback: náº¿u khÃ´ng detect Ä‘Æ°á»£c chÃ¢n mÃ y
#             print("âš ï¸ Cannot detect eyebrow landmarks, using fallback method")
#             x1, y1, x2, y2 = face_info['bbox']
#             face_height = y2 - y1
#             # ChÃ¢n mÃ y thÆ°á»ng á»Ÿ khoáº£ng 40-45% tá»« trÃªn xuá»‘ng
#             eyebrow_y = int(y1 + face_height * 0.45)
#             return eyebrow_y
#
#         except Exception as e:
#             print(f"âŒ Error in eyebrow detection: {e}")
#             # Fallback
#             x1, y1, x2, y2 = face_info['bbox']
#             face_height = y2 - y1
#             return int(y1 + face_height * 0.45)
#
#     def create_face_protection_mask(
#             self,
#             image: np.ndarray,
#             face_info: Dict
#     ) -> np.ndarray:
#         """Táº¡o mask báº£o vá»‡ tá»« CHÃ‚N MÃ€Y trá»Ÿ XUá»NG"""
#         h, w = image.shape[:2]
#
#         # 1. Khá»Ÿi táº¡o mask TRáº®NG toÃ n bá»™ (thay Ä‘á»•i toÃ n bá»™)
#         mask = np.ones((h, w), dtype=np.uint8) * 255
#
#         # 2. Láº¥y vá»‹ trÃ­ chÃ¢n mÃ y (gá»i hÃ m get_forehead_position Ä‘Ã£ Ä‘Æ°á»£c sá»­a)
#         eyebrow_line_y = self.get_forehead_position(image, face_info)
#
#         # 3. Láº¥y landmarks máº·t Ä‘á»ƒ táº¡o mask chÃ­nh xÃ¡c Tá»ª CHÃ‚N MÃ€Y XUá»NG
#         try:
#             if hasattr(self, 'face_mesh'):
#                 rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
#                 results = self.face_mesh.process(rgb_image)
#
#                 if results.multi_face_landmarks:
#                     face_landmarks = results.multi_face_landmarks[0]
#
#                     # Táº¡o mask cho pháº§n máº·t Tá»ª CHÃ‚N MÃ€Y XUá»NG
#                     face_points = []
#
#                     # Láº¥y cÃ¡c landmarks xung quanh máº·t CHá»ˆ Tá»ª CHÃ‚N MÃ€Y XUá»NG
#                     jaw_indices = [152, 148, 176, 149, 150, 136, 172, 58, 132, 93]
#                     cheek_indices = [116, 117, 118, 119, 100, 47, 126, 209, 49]
#                     mouth_indices = [61, 146, 91, 181, 84, 17, 314, 405, 321, 375]
#                     nose_indices = [1, 2, 98, 327]
#                     eye_indices = [33, 133, 157, 158, 159, 160, 161, 173, 263, 362, 386, 387, 388, 389, 390, 466]
#
#                     all_face_indices = jaw_indices + cheek_indices + mouth_indices + nose_indices + eye_indices
#
#                     for idx in all_face_indices:
#                         if len(face_landmarks.landmark) > idx:
#                             lm = face_landmarks.landmark[idx]
#                             px = int(lm.x * w)
#                             py = int(lm.y * h)
#                             # CHá»ˆ láº¥y points Tá»ª CHÃ‚N MÃ€Y XUá»NG (y >= eyebrow_line_y)
#                             if py >= eyebrow_line_y:
#                                 face_points.append((px, py))
#
#                     if len(face_points) > 3:
#                         # Táº¡o convex hull tá»« cÃ¡c points
#                         points_array = np.array(face_points, dtype=np.int32)
#                         hull = cv2.convexHull(points_array)
#
#                         # Váº½ convex hull = ÄEN (0) trÃªn mask táº¡m
#                         face_region = np.zeros((h, w), dtype=np.uint8)
#                         cv2.fillConvexPoly(face_region, hull, 255)
#
#                         # QUAN TRá»ŒNG: XÃ³a pháº§n TRÃŠN chÃ¢n mÃ y khá»i face_region
#                         # Chá»‰ giá»¯ láº¡i pháº§n tá»« chÃ¢n mÃ y xuá»‘ng
#                         face_region[:eyebrow_line_y, :] = 0
#
#                         # Trá»« vÃ¹ng máº·t ra khá»i mask tá»•ng
#                         mask = cv2.subtract(mask, face_region)
#
#                         print(f"âœ… Created face mask FROM EYEBROW DOWN using {len(face_points)} landmarks")
#                         print(f"   Protected area: y >= {eyebrow_line_y}")
#                         print(f"   Change area: y < {eyebrow_line_y} (forehead & hair)")
#
#                         # Debug
#                         if self.config.DEBUG_MODE:
#                             cv2.imwrite(f'{self.config.DEBUG_DIR}/face_mask_eyeb7ow_down.png', mask)
#
#                             # Táº¡o visualization
#                             debug_img = image.copy()
#                             mask_bgr = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)
#                             # TÃ´ Ä‘á» vÃ¹ng Ä‘Æ°á»£c báº£o vá»‡ (máº·t)
#                             protected_area = mask == 0
#                             debug_img[protected_area] = cv2.addWeighted(
#                                 debug_img[protected_area], 0.7,
#                                 np.array([0, 0, 255], dtype=np.uint8), 0.3, 0
#                             )[protected_area]
#
#                             # Váº½ Ä‘Æ°á»ng chÃ¢n mÃ y
#                             cv2.line(debug_img, (0, eyebrow_line_y), (w, eyebrow_line_y),
#                                      (0, 255, 0), 2)
#                             cv2.putText(debug_img, 'EYEBROW LINE - PROTECT BELOW',
#                                         (10, eyebrow_line_y - 10),
#                                         cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
#                             cv2.putText(debug_img, 'CHANGE ABOVE (Forehead & Hair)',
#                                         (10, 30),
#                                         cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
#                             cv2.imwrite(f'{self.config.DEBUG_DIR}/protection_visualization.png', debug_img)
#
#                         return mask
#         except Exception as e:
#             print(f"âš ï¸ Landmark-based face mask failed: {e}")
#
#         # 4. Fallback: dÃ¹ng phÆ°Æ¡ng phÃ¡p cÆ¡ báº£n báº£o vá»‡ tá»« chÃ¢n mÃ y xuá»‘ng
#         print("âš ï¸ Using fallback face protection method (protect from eyebrow down)")
#
#         x1, y1, x2, y2 = face_info['bbox']
#         face_width = x2 - x1
#         face_height = y2 - y1
#
#         # Táº¡o mask táº¡m cho vÃ¹ng máº·t Tá»ª CHÃ‚N MÃ€Y XUá»NG
#         face_region = np.zeros((h, w), dtype=np.uint8)
#
#         # Váº½ hÃ¬nh oval bao quanh máº·t (tá»« eyebrow_line_y xuá»‘ng)
#         center_x = (x1 + x2) // 2
#         # Center y tá»« eyebrow_line_y Ä‘áº¿n cáº±m
#         center_y = (eyebrow_line_y + y2) // 2
#         axes_x = int(face_width * 0.5)
#         axes_y = int((y2 - eyebrow_line_y) * 0.6)
#
#         cv2.ellipse(
#             face_region,
#             (center_x, center_y),
#             (axes_x, axes_y),
#             0, 0, 360,
#             255,
#             -1
#         )
#
#         # QUAN TRá»ŒNG: XÃ³a pháº§n TRÃŠN chÃ¢n mÃ y
#         face_region[:eyebrow_line_y, :] = 0
#
#         # Trá»« vÃ¹ng máº·t
#         mask = cv2.subtract(mask, face_region)
#
#         # LÃ m má»m viá»n
#         mask = cv2.GaussianBlur(mask, (21, 21), 10)
#         _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)
#
#         return mask

class HairMaskGenerator:
    """Táº¡o mask vÃ¹ng tÃ³c (phÃ­a trÃªn trÃ¡n)"""

    def __init__(self, config: HairStyleConfig):
        self.config = config

    def create_hair_mask(
            self,
            image: np.ndarray,
            face_info: Dict,
            hairline_y: int
    ) -> np.ndarray:
        """Táº¡o mask tÃ³c CHÃNH XÃC theo Ä‘Æ°á»ng chÃ¢n tÃ³c"""
        h, w = image.shape[:2]

        # 1. Khá»Ÿi táº¡o mask Ä‘en
        mask = np.zeros((h, w), dtype=np.uint8)

        if face_info is None:
            # Fallback
            cv2.rectangle(mask, (0, 0), (w, h // 2), 255, -1)
            return mask

        # 2. Láº¥y thÃ´ng tin máº·t
        x1, y1, x2, y2 = face_info['bbox']
        face_width = x2 - x1
        face_height = y2 - y1

        print(f"ğŸ¯ Creating hair mask with hairline_y={hairline_y}")

        # 3. Táº¡o mask tÃ³c theo hÃ¬nh dáº¡ng tá»± nhiÃªn hÆ¡n
        # VÃ¹ng tÃ³c chÃ­nh: tá»« Ä‘áº§u áº£nh Ä‘áº¿n hairline_y

        # Táº¡o polygon cho vÃ¹ng tÃ³c (hÃ¬nh oval hÆ¡n)
        points = []

        # Äiá»ƒm trÃªn cÃ¹ng (giá»¯a)
        points.append((w // 2, 0))

        # Äiá»ƒm bÃªn trÃ¡i (trÃªn)
        points.append((0, int(hairline_y * 0.3)))

        # Äiá»ƒm bÃªn trÃ¡i (dÆ°á»›i)
        points.append((0, hairline_y))

        # Äiá»ƒm dÆ°á»›i giá»¯a (á»Ÿ hairline)
        points.append((w // 2, hairline_y))

        # Äiá»ƒm bÃªn pháº£i (dÆ°á»›i)
        points.append((w, hairline_y))

        # Äiá»ƒm bÃªn pháº£i (trÃªn)
        points.append((w, int(hairline_y * 0.3)))

        # Váº½ polygon
        pts = np.array(points, dtype=np.int32)
        cv2.fillPoly(mask, [pts], 255)

        # 4. ThÃªm vÃ¹ng tÃ³c hai bÃªn (mÃ¡i tÃ³c)
        # BÃªn trÃ¡i
        left_width = int(face_width * 0.4)
        left_x1 = max(0, x1 - left_width)
        left_x2 = x1
        left_y1 = max(0, hairline_y - int(face_height * 0.3))
        left_y2 = min(h, hairline_y + int(face_height * 0.2))
        cv2.rectangle(mask, (left_x1, left_y1), (left_x2, left_y2), 255, -1)

        # BÃªn pháº£i
        right_x1 = x2
        right_x2 = min(w, x2 + left_width)
        cv2.rectangle(mask, (right_x1, left_y1), (right_x2, left_y2), 255, -1)

        # 5. Äáº£m báº£o mask Ä‘á»§ lá»›n
        mask_ratio = np.sum(mask == 255) / mask.size
        print(f"ğŸ” Initial hair mask ratio: {mask_ratio:.2%}")

        if mask_ratio < 0.25:
            print("âš ï¸ Hair mask too small, expanding...")
            # ThÃªm vÃ¹ng trÃªn
            top_height = int(h * 0.35)
            mask[0:top_height, :] = 255

        # 6. Má»Ÿ rá»™ng mask nháº¹
        kernel_size = max(15, int(min(h, w) * 0.03))
        kernel = np.ones((kernel_size, kernel_size), np.uint8)
        mask = cv2.dilate(mask, kernel, iterations=1)

        # 7. LÃ m má»m edges
        mask = cv2.GaussianBlur(mask, (21, 21), 11)
        _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)

        final_ratio = np.sum(mask == 255) / mask.size
        print(f"âœ… Final hair mask ratio: {final_ratio:.2%}")

        # Debug
        if self.config.DEBUG_MODE:
            overlay = image.copy()
            overlay[mask > 0] = overlay[mask > 0] * 0.7 + np.array([0, 255, 0]) * 0.3
            cv2.line(overlay, (0, hairline_y), (w, hairline_y), (0, 0, 255), 3)
            cv2.imwrite(f'{self.config.DEBUG_DIR}/precise_hair_mask.png', mask)
            cv2.imwrite(f'{self.config.DEBUG_DIR}/precise_hair_overlay.png', overlay)

        return mask

class HairStyleGeneratorSD15:
    """SD1.5 Inpainting Generator - ChÃ­nh"""

    def __init__(self, config: Optional[HairStyleConfig] = None):
        self.config = config or HairStyleConfig()
        self.device = self.config.DEVICE

        logger.info(f"ğŸ¯ Initializing SD1.5 Inpainting Generator")
        logger.info(f"ğŸ“± Device: {self.device}")
        logger.info(f"ğŸ–¼ï¸  Image size: {self.config.IMAGE_SIZE}")

        # Initialize components
        self.face_detector = FaceDetectorMediaPipe(self.config)
        self.hair_mask_generator = HairMaskGenerator(self.config)
        self.pipe = None

        # Load models
        self._load_models()

        logger.info("âœ… SD1.5 Generator initialized successfully!")

    def _load_models(self):
        """Load SD1.5 Inpainting pipeline"""
        logger.info("ğŸ“¦ Loading SD1.5 Inpainting model...")

        try:
            # Load pipeline
            self.pipe = StableDiffusionInpaintPipeline.from_pretrained(
                self.config.SD15_INPAINT_MODEL,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                safety_checker=None,
                requires_safety_checker=False,
            )

            self.pipe = self.pipe.to(self.device)

            # Optimizations
            if self.device == "cuda":
                if self.config.USE_XFORMERS:
                    try:
                        self.pipe.enable_xformers_memory_efficient_attention()
                        logger.info("âœ… xFormers enabled")
                    except:
                        logger.warning("âš ï¸ xFormers not available")

                # Enable memory optimization
                self.pipe.enable_attention_slicing()

            # Scheduler - UniPC nhanh vÃ  á»•n Ä‘á»‹nh
            self.pipe.scheduler = UniPCMultistepScheduler.from_config(
                self.pipe.scheduler.config
            )

            logger.info(f"âœ… Model loaded: {self.config.SD15_INPAINT_MODEL}")

        except Exception as e:
            logger.error(f"âŒ Error loading model: {e}")
            raise

    def preprocess_image(
            self,
            image: np.ndarray
    ) -> Tuple[Image.Image, Image.Image, Dict]:
        """
        Preprocess áº£nh vÃ  táº¡o mask
        Returns: (pil_image, pil_mask, processing_info)
        """
        target_w, target_h = self.config.IMAGE_SIZE  # 512x512
        h, w = image.shape[:2]

        # Resize vá»›i giá»¯ tá»‰ lá»‡
        scale = min(target_w / w, target_h / h)
        new_w, new_h = int(w * scale), int(h * scale)

        resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)

        # Pad vá» 512x512
        pad_w = (target_w - new_w) // 2
        pad_h = (target_h - new_h) // 2

        padded = cv2.copyMakeBorder(
            resized,
            pad_h, target_h - new_h - pad_h,
            pad_w, target_w - new_w - pad_w,
            cv2.BORDER_CONSTANT,
            value=(255, 255, 255)  # White background
        )

        # Convert to PIL
        rgb_image = cv2.cvtColor(padded, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(rgb_image)

        # Face detection
        face_info = self.face_detector.get_face_info(padded)

        if face_info:
            logger.info(f"âœ… Face detected: {face_info['bbox']}")

            # 1. XÃ¡c Ä‘á»‹nh Ä‘Æ°á»ng chia tÃ³c/máº·t
            hairline_y = self.face_detector.get_forehead_position(padded, face_info)

            # 2. Táº¡o mask vÃ¹ng tÃ³c
            hair_mask = self.hair_mask_generator.create_hair_mask(
                padded, face_info, hairline_y
            )

            # 3. Táº¡o mask báº£o vá»‡ máº·t
            face_mask = self.face_detector.create_face_protection_mask(
                padded, face_info
            )

            # 4. Káº¿t há»£p mask:
            # face_mask: máº·t = 255 (tráº¯ng = báº£o vá»‡), khÃ¡c = 0 (Ä‘en)
            # hair_mask: tÃ³c = 255 (tráº¯ng = thay Ä‘á»•i), khÃ¡c = 0

            # Logic: Chá»‰ thay Ä‘á»•i vÃ¹ng tÃ³c KHÃ”NG overlap vá»›i máº·t
            # final_mask = hair_mask AND (NOT face_mask)
            not_face_mask = cv2.bitwise_not(face_mask)  # Äáº£o ngÆ°á»£c: máº·t=0, khÃ¡c=255
            final_mask = cv2.bitwise_and(hair_mask, not_face_mask)

            # Äáº£m báº£o vÃ¹ng máº·t hoÃ n toÃ n Ä‘en (0)
            x1, y1, x2, y2 = face_info['bbox']
            padding = 5
            x1 = max(0, x1 - padding)
            y1 = max(0, y1 - padding)
            x2 = min(final_mask.shape[1], x2 + padding)
            y2 = min(final_mask.shape[0], y2 + padding)
            final_mask[y1:y2, x1:x2] = 0

            # Statistics
            mask_stats = {
                'hair_pixels': int(np.sum(hair_mask == 255)),
                'face_protected': int(np.sum(face_mask == 255)),
                'final_changed': int(np.sum(final_mask == 255)),
                'change_percentage': f"{np.sum(final_mask == 255) / final_mask.size * 100:.1f}%",
                'hairline_y': hairline_y,
            }

            logger.info(f"ğŸ“Š Mask stats: {mask_stats['change_percentage']} changed")

        else:
            logger.warning("âš ï¸ No face detected, using fallback mask")
            # Fallback: mask ná»­a trÃªn áº£nh
            final_mask = np.zeros((padded.shape[0], padded.shape[1]), dtype=np.uint8)
            cv2.rectangle(final_mask, (0, 0), (padded.shape[1], padded.shape[0] // 2), 255, -1)
            mask_stats = {}

        # Convert mask to PIL
        pil_mask = Image.fromarray(final_mask).convert("L")

        # Debug: save final mask
        if self.config.DEBUG_MODE:
            cv2.imwrite(f'{self.config.DEBUG_DIR}/final7_mask.png', final_mask)

        processing_info = {
            'original_size': (w, h),
            'resized_size': (new_w, new_h),
            'padded_size': (target_w, target_h),
            'has_face': face_info is not None,
            'face_bbox': face_info['bbox'] if face_info else None,
            'mask_stats': mask_stats,
        }

        return pil_image, pil_mask, processing_info

    def get_style_prompt(
            self,
            style_name: str
    ) -> Tuple[str, str]:
        """Láº¥y prompt cho style tá»« config"""
        from app.config.hair_config import HairStylePrompts

        if style_name not in HairStylePrompts.HAIR_STYLES:
            raise ValueError(f"Style '{style_name}' not found")

        style_config = HairStylePrompts.HAIR_STYLES[style_name]

        # Simplify prompt cho SD1.5
        base_prompt = style_config['prompt']
        base_negative = style_config['negative']

        # Láº¥y tá»« khÃ³a chÃ­nh
        prompt_words = base_prompt.replace('hair transformation ONLY', '').replace('PRESERVE EVERYTHING ELSE EXACTLY',
                                                                                   '')
        prompt_words = prompt_words.split(',')

        # Giá»¯ 3 pháº§n Ä‘áº§u + face preservation
        simple_prompt = ', '.join(prompt_words[:3])
        simple_prompt += ", keep original face exactly the same, same person identity"

        # Negative prompt Ä‘Æ¡n giáº£n
        simple_negative = "different face, changed face, blurry face, ugly face, deformed face"

        return simple_prompt, simple_negative

    def generate_single_style(
            self,
            image: np.ndarray,
            style_name: str,
            seed: Optional[int] = None,
            num_steps: Optional[int] = None,
            denoising_strength: Optional[float] = None,
            guidance_scale: Optional[float] = None
    ) -> Dict:
        # Clear cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        start_time = time.time()
        logger.info(f"ğŸ¨ Generating: {style_name}")

        # 1. Preprocess
        pil_image, pil_mask, processing_info = self.preprocess_image(image)

        mask_array = np.array(pil_mask)
        mask_ratio = np.sum(mask_array == 255) / mask_array.size

        print("=" * 60)
        print(f"ğŸ”´ MASK RATIO: {mask_ratio:.2%}")
        print("=" * 60)

        # 2. AUTO-ADJUST STRENGTH BASED ON MASK SIZE - FIXED!
        if mask_ratio < 0.05:  # <5%
            strength = 0.98  # Cá»°C CAO
        elif mask_ratio < 0.1:  # 5-10%
            strength = 0.95
        elif mask_ratio < 0.2:  # 10-20%
            strength = 0.9
        elif mask_ratio < 0.3:  # 20-30%
            strength = 0.85
        else:
            strength = denoising_strength or self.config.DENOISING_STRENGTH

        print(f"ğŸ¯ AUTO-ADJUSTED STRENGTH: {strength}")

        # 3. EXPAND MASK IF TOO SMALL
        if mask_ratio < 0.2:  # Náº¿u mask < 20%
            print("âš ï¸ Mask too small, STRONGLY expanding...")

            # Dilate máº¡nh hÆ¡n
            kernel_size = max(30, int(min(mask_array.shape) * 0.1))  # 10% cá»§a kÃ­ch thÆ°á»›c
            kernel = np.ones((kernel_size, kernel_size), np.uint8)

            # Dilate nhiá»u láº§n
            expanded_mask = cv2.dilate(mask_array, kernel, iterations=3)

            # ThÃªm vÃ¹ng lá»›n
            height, width = expanded_mask.shape
            expanded_mask[0:int(height * 0.4), :] = 255  # 40% trÃªn cÃ¹ng
            expanded_mask[:, 0:int(width * 0.2)] = 255  # 20% trÃ¡i
            expanded_mask[:, int(width * 0.8):] = 255  # 20% pháº£i

            # Update mask
            mask_array = expanded_mask
            pil_mask = Image.fromarray(mask_array).convert("L")

            # TÃ­nh láº¡i ratio
            new_ratio = np.sum(mask_array == 255) / mask_array.size
            print(f"âœ… STRONGLY Expanded mask: {mask_ratio:.2%} â†’ {new_ratio:.2%}")
            mask_ratio = new_ratio

            # LÆ°u mask Ä‘á»ƒ debug


        # 4. SIMPLIFY PROMPT - QUAN TRá»ŒNG!
        # Thay vÃ¬ dÃ¹ng prompt tá»« config, dÃ¹ng prompt ÄÆ N GIáº¢N
        prompt_map = {
            "blue_hair": "BLUE HAIR, vibrant blue color, colorful hairstyle",
            "man_bun": "MAN BUN, long hair tied up in bun, top knot",
            "short_undercut": "SHORT UNDERCUT, shaved sides, fade haircut",
            "slicked_back": "SLICKED BACK HAIR, smooth combed back style",
            "curly_afro": "CURLY AFRO, natural curls, textured hair",
            "korean_style": "KOREAN HAIRSTYLE, K-pop style, textured fringe",
            "side_part": "SIDE PART, neat combed hair, professional style",
            "bob_cut": "BOB CUT, shoulder length hair, feminine style",
            "pixie_cut": "PIXIE CUT, short layered hair, feminine crop",
            "buzz_cut": "BUZZ CUT, very short hair, military style"
        }

        base_prompt = prompt_map.get(style_name, f"{style_name} hairstyle")
        positive_prompt = f"{base_prompt}, keep face exactly the same, same person identity"
        negative_prompt = "different face, changed face, blurry face, ugly face"

        print(f"ğŸ“ SIMPLE PROMPT: {positive_prompt}")

        # 5. PARAMETERS - DÃ™NG strength ÄÃƒ TÃNH
        num_steps = num_steps or 40  # TÄƒng steps
        guidance = guidance_scale or 8.0  # TÄƒng guidance

        print(f"âš™ï¸ FINAL PARAMS: strength={strength}, steps={num_steps}, guidance={guidance}")

        # 6. Seed
        if seed is None:
            seed = int(time.time() * 1000) % 1000000

        generator = torch.Generator(device=self.device).manual_seed(seed)

        # 7. GENERATE - DÃ™NG strength (khÃ´ng pháº£i denoising)
        print("ğŸš€ Running pipeline with STRONG settings...")

        output = self.pipe(
            prompt=positive_prompt,
            negative_prompt=negative_prompt,
            image=pil_image,
            mask_image=pil_mask,
            strength=strength,  # QUAN TRá»ŒNG: DÃ¹ng strength Ä‘Ã£ tÃ­nh
            num_inference_steps=num_steps,
            guidance_scale=guidance,
            generator=generator,
        )

        result_image = output.images[0]

        # 8. DEBUG: LÆ°u áº£nh Ä‘á»ƒ kiá»ƒm tra


        # 9. Crop vá» kÃ­ch thÆ°á»›c gá»‘c
        if processing_info.get('padded_size'):
            result_array = np.array(result_image)
            padded_w, padded_h = processing_info['padded_size']
            new_w, new_h = processing_info['resized_size']
            pad_w = (padded_w - new_w) // 2
            pad_h = (padded_h - new_h) // 2

            cropped = result_array[
                      pad_h:pad_h + new_h,
                      pad_w:pad_w + new_w
                      ]
            result_image = Image.fromarray(cropped)

        elapsed_time = time.time() - start_time
        print(f"âœ… Generated in {elapsed_time:.2f}s")
        print("=" * 60)

        return {
            'result': result_image,
            'mask': pil_mask,
            'processing_info': processing_info,
            'prompts': {
                'positive': positive_prompt,
                'negative': negative_prompt
            },
            'settings': {
                'seed': seed,
                'steps': num_steps,
                'strength': strength,  # Äá»•i tá»« denoising â†’ strength
                'guidance': guidance,
                'mask_ratio': f"{mask_ratio:.1%}",
                'model': 'SD1.5'
            }
        }


# Singleton instance
_generator_instance = None


def get_hair_generator() -> HairStyleGeneratorSD15:
    """Get generator instance (singleton)"""
    global _generator_instance

    if _generator_instance is None:
        logger.info("Creating SD1.5 Hair Generator instance")
        _generator_instance = HairStyleGeneratorSD15()

    return _generator_instance


def cleanup_generator():
    """Cleanup generator vÃ  free memory"""
    global _generator_instance

    if _generator_instance is not None:
        del _generator_instance
        _generator_instance = None

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        logger.info("Generator cleaned up")


"""
CÃ€I Äáº¶T:
pip install mediapipe diffusers transformers accelerate torch torchvision

USAGE:
1. Basic usage:
   from hairstyle_service_sd15 import get_hair_generator
   import cv2

   generator = get_hair_generator()
   image = cv2.imread("input.jpg")

   result = generator.generate_single_style(
       image=image,
       style_name="man_bun",
       denoising_strength=0.8,
       seed=42
   )

   result['result'].save("output.jpg")

2. Check debug masks in ./debug_masks/:

   - final_7mask.png: Mask cuá»‘i cÃ¹ng


3. Tips:
   - Náº¿u mask nhá» (<10%): tÄƒng denoising_strength lÃªn 0.85
   - Náº¿u káº¿t quáº£ blurry: tÄƒng num_inference_steps lÃªn 40-50
   - Náº¿u khÃ´ng thay Ä‘á»•i: check debug masks xem cÃ³ Ä‘Ãºng khÃ´ng
"""

if __name__ == "__main__":
    # Test script
    import sys

    if len(sys.argv) > 1:
        image_path = sys.argv[1]
        style = sys.argv[2] if len(sys.argv) > 2 else "man_bun"

        image = cv2.imread(image_path)
        if image is not None:
            generator = get_hair_generator()
            result = generator.generate_single_style(
                image=image,
                style_name=style,
                denoising_strength=0.8,
                seed=42
            )
            result['result'].save(f"output_{style}.jpg")
            print(f"âœ… Saved to output_{style}.jpg")

            # Save mask
            result['mask'].save(f"mask_{style}.png")
            print(f"âœ… Mask saved to mask_{style}.png")
        else:
            print(f"âŒ Cannot read image: {image_path}")
    else:
        print("Usage: python hairstyle_service_sd15.py <image_path> [style_name]")


# """
# Hair Style Generation Service - SDXL + Inpainting vá»›i MediaPipe Face Detection
# FIXED: Lá»—i FACE_PROTECTION_PADDING
# """
#
# import torch
# import cv2
# import numpy as np
# from PIL import Image
# from typing import List, Dict, Tuple, Optional, Union
# import logging
# import time
# import mediapipe as mp
#
# from diffusers import (
#     StableDiffusionXLInpaintPipeline,
#     AutoPipelineForInpainting,
#     UniPCMultistepScheduler
# )
#
# logger = logging.getLogger(__name__)
#
#
# class HairStyleConfig:
#     """Config cho SDXL + Inpainting vá»›i MediaPipe"""
#     SDXL_INPAINT_MODEL = "diffusers/stable-diffusion-xl-1.0-inpainting-0.1"
#     DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
#     USE_XFORMERS = True
#
#     # Generation settings
#     NUM_INFERENCE_STEPS = 40
#     GUIDANCE_SCALE = 9.0
#     IMAGE_SIZE = (1024, 1024)
#
#     # âœ… CRITICAL: Denoising pháº£i cao Ä‘á»ƒ tháº¥y thay Ä‘á»•i tÃ³c
#     DENOISING_STRENGTH = 0.8
#
#     MASK_DILATE = 20  # TÄƒng Ä‘á»ƒ má»Ÿ rá»™ng mask tÃ³c
#     FACE_PROTECTION_PADDING = 0.2  # Padding quanh máº·t Ä‘á»ƒ báº£o vá»‡
#
#
# class FaceDetectorMediaPipe:
#     """Face detection vá»›i MediaPipe - NHANH & CHÃNH XÃC"""
#
#     def __init__(self, config: HairStyleConfig = None):
#         logger.info("Initializing MediaPipe Face Detection...")
#
#         # LÆ°u config
#         self.config = config or HairStyleConfig()
#
#         # Khá»Ÿi táº¡o MediaPipe Face Detection
#         mp_face_detection = mp.solutions.face_detection
#         self.face_detection = mp_face_detection.FaceDetection(
#             model_selection=1,  # 0=short-range, 1=full-range
#             min_detection_confidence=0.5
#         )
#
#         # Khá»Ÿi táº¡o MediaPipe Face Mesh (cho landmarks chÃ­nh xÃ¡c hÆ¡n)
#         mp_face_mesh = mp.solutions.face_mesh
#         self.face_mesh = mp_face_mesh.FaceMesh(
#             static_image_mode=True,
#             max_num_faces=1,
#             refine_landmarks=True,
#             min_detection_confidence=0.5
#         )
#
#         logger.info("âœ“ MediaPipe Face Detection initialized!")
#
#     def get_face_info(self, image: np.ndarray) -> Optional[Dict]:
#         """Get face bounding box vÃ  landmarks vá»›i MediaPipe"""
#         try:
#             # Chuyá»ƒn BGR â†’ RGB
#             rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
#             h, w = image.shape[:2]
#
#             # 1. Face Detection Ä‘á»ƒ láº¥y bbox
#             detection_results = self.face_detection.process(rgb_image)
#
#             if detection_results.detections:
#                 # Láº¥y face cÃ³ confidence cao nháº¥t
#                 detection = detection_results.detections[0]
#                 bbox = detection.location_data.relative_bounding_box
#
#                 # Convert relative coordinates to absolute
#                 x = int(bbox.xmin * w)
#                 y = int(bbox.ymin * h)
#                 width = int(bbox.width * w)
#                 height = int(bbox.height * h)
#
#                 bbox_abs = [x, y, x + width, y + height]
#
#                 # 2. Face Mesh Ä‘á»ƒ láº¥y landmarks chÃ­nh xÃ¡c
#                 mesh_results = self.face_mesh.process(rgb_image)
#
#                 landmarks = []
#                 forehead_point = None
#
#                 if mesh_results.multi_face_landmarks:
#                     face_landmarks = mesh_results.multi_face_landmarks[0]
#
#                     # Extract landmarks (468 points)
#                     for landmark in face_landmarks.landmark:
#                         px = int(landmark.x * w)
#                         py = int(landmark.y * h)
#                         landmarks.append([px, py])
#
#                     # TÃ¬m Ä‘iá»ƒm trÃªn trÃ¡n (forehead)
#                     # Landmark 10: Ä‘iá»ƒm giá»¯a trÃ¡n
#                     if len(landmarks) > 10:
#                         forehead_point = landmarks[10]
#
#                 return {
#                     'bbox': bbox_abs,
#                     'landmarks': landmarks,
#                     'forehead_point': forehead_point,
#                     'detection_confidence': detection.score[0]
#                 }
#
#             return None
#
#         except Exception as e:
#             logger.error(f"Error in MediaPipe face detection: {e}")
#             return None
#
#     def get_forehead_position(self, image: np.ndarray, face_info: Dict) -> Tuple[int, int]:
#         """XÃ¡c Ä‘á»‹nh vá»‹ trÃ­ Ä‘Æ°á»ng ngang cá»§a trÃ¡n (Ä‘Æ°á»ng phÃ¢n chia tÃ³c/máº·t)"""
#         h, w = image.shape[:2]
#         x1, y1, x2, y2 = face_info['bbox']
#         face_height = y2 - y1
#
#         # Chiáº¿n lÆ°á»£c: láº¥y vá»‹ trÃ­ tá»« landmarks hoáº·c tÃ­nh theo tá»‰ lá»‡
#         if face_info.get('forehead_point'):
#             # DÃ¹ng landmark trÃ¡n
#             forehead_y = face_info['forehead_point'][1]
#         else:
#             # Æ¯á»›c tÃ­nh: trÃ¡n báº¯t Ä‘áº§u tá»« 1/3 trÃªn cá»§a máº·t
#             forehead_y = int(y1 + face_height * 0.2)
#
#         # ÄÆ°á»ng ngang chia tÃ³c/máº·t: lÃ¹i lÃªn má»™t chÃºt tá»« trÃ¡n
#         hairline_y = max(0, forehead_y - int(face_height * 0.1))
#
#         return hairline_y
#
#     def create_face_protection_mask(
#         self,
#         image: np.ndarray,
#         face_info: Dict
#     ) -> np.ndarray:
#         """Táº¡o mask báº£o vá»‡ khuÃ´n máº·t (tá»« trÃ¡n xuá»‘ng)"""
#         h, w = image.shape[:2]
#
#         # 1. Táº¡o mask Ä‘en (khÃ´ng báº£o vá»‡ gÃ¬)
#         mask = np.zeros((h, w), dtype=np.uint8)
#
#         # 2. XÃ¡c Ä‘á»‹nh vÃ¹ng máº·t cáº§n báº£o vá»‡
#         x1, y1, x2, y2 = face_info['bbox']
#
#         # âœ… FIX: Láº¥y padding tá»« config
#         if hasattr(self, 'config') and hasattr(self.config, 'FACE_PROTECTION_PADDING'):
#             padding = int((x2 - x1) * self.config.FACE_PROTECTION_PADDING)
#         else:
#             padding = int((x2 - x1) * 0.2)  # Fallback
#
#         x1_protect = max(0, x1 - padding)
#         y1_protect = max(0, y1 - padding // 2)  # Ãt padding trÃªn
#         x2_protect = min(w, x2 + padding)
#         y2_protect = min(h, y2 + padding)
#
#         # XÃ¡c Ä‘á»‹nh Ä‘Æ°á»ng chia tÃ³c/máº·t
#         hairline_y = self.get_forehead_position(image, face_info)
#
#         # 3. Váº½ mask báº£o vá»‡:
#         # - Máº·t (tá»« hairline_y xuá»‘ng): WHITE (255) = Báº¢O Vá»†
#         # - TÃ³c (trÃªn hairline_y): BLACK (0) = KHÃ”NG Báº¢O Vá»†
#
#         # Váº½ hÃ¬nh oval bao quanh máº·t (tá»« hairline_y xuá»‘ng)
#         center_y = (hairline_y + y2_protect) // 2
#         center_x = (x1_protect + x2_protect) // 2
#         axes_x = (x2_protect - x1_protect) // 2
#         axes_y = (y2_protect - hairline_y) // 2
#
#         cv2.ellipse(
#             mask,
#             (center_x, center_y),
#             (axes_x, axes_y),
#             0, 0, 360,
#             255, -1
#         )
#
#         # ThÃªm pháº§n dÆ°á»›i cáº±m (Ä‘á»ƒ cháº¯c cháº¯n)
#         chin_y_start = max(hairline_y, int((y1 + y2) // 2))
#         cv2.rectangle(
#             mask,
#             (x1_protect, chin_y_start),
#             (x2_protect, y2_protect),
#             255, -1
#         )
#
#         # Blur Ä‘á»ƒ lÃ m má»m edges
#         mask = cv2.GaussianBlur(mask, (31, 31), 15)
#
#         # Threshold vá» binary
#         _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)
#
#         # DEBUG: LÆ°u mask
#         debug_dir = "./debug_masks"
#         import os
#         os.makedirs(debug_dir, exist_ok=True)
#         cv2.imwrite(f'{debug_dir}/debug_face_protection_mask1.png', mask)
#
#         return mask
#
#
# class HairMaskGenerator:
#     """Táº¡o mask vÃ¹ng tÃ³c (phÃ­a trÃªn trÃ¡n)"""
#
#     def __init__(self, config: HairStyleConfig):
#         self.config = config
#
#     def create_hair_mask(
#         self,
#         image: np.ndarray,
#         face_info: Dict,
#         face_detector: 'FaceDetectorMediaPipe'
#     ) -> np.ndarray:
#         """Táº¡o mask cho vÃ¹ng tÃ³c (phÃ­a TRÃŠN trÃ¡n)"""
#         h, w = image.shape[:2]
#
#         # 1. Khá»Ÿi táº¡o mask Ä‘en
#         mask = np.zeros((h, w), dtype=np.uint8)
#
#         if face_info is None:
#             # KhÃ´ng cÃ³ face: mask toÃ n bá»™ ná»­a trÃªn
#             cv2.rectangle(mask, (0, 0), (w, h // 2), 255, -1)
#             return mask
#
#         # 2. XÃ¡c Ä‘á»‹nh Ä‘Æ°á»ng chia tÃ³c/máº·t
#         hairline_y = face_detector.get_forehead_position(image, face_info)
#
#         # 3. Táº¡o mask vÃ¹ng tÃ³c (phÃ­a TRÃŠN hairline_y)
#         x1, y1, x2, y2 = face_info['bbox']
#         face_width = x2 - x1
#
#         # VÃ¹ng tÃ³c chÃ­nh (phÃ­a trÃªn trÃ¡n)
#         hair_x1 = max(0, int(x1 - face_width * 0.5))
#         hair_x2 = min(w, int(x2 + face_width * 0.5))
#         hair_y1 = max(0, hairline_y - int(face_width * 0.8))  # Má»Ÿ rá»™ng lÃªn trÃªn
#         hair_y2 = hairline_y  # Dá»«ng á»Ÿ Ä‘Æ°á»ng chia
#
#         cv2.rectangle(
#             mask,
#             (hair_x1, hair_y1),
#             (hair_x2, hair_y2),
#             255, -1
#         )
#
#         # 4. ThÃªm vÃ¹ng tÃ³c hai bÃªn (cho tÃ³c dÃ i/phá»“ng)
#         side_width = int(face_width * 0.4)
#         side_height = int(face_width * 0.8)
#
#         # BÃªn trÃ¡i
#         left_x1 = max(0, x1 - side_width)
#         left_x2 = x1
#         left_y1 = max(0, hairline_y - side_height)
#         left_y2 = min(h, hairline_y + side_height // 2)
#         cv2.rectangle(mask, (left_x1, left_y1), (left_x2, left_y2), 255, -1)
#
#         # BÃªn pháº£i
#         right_x1 = x2
#         right_x2 = min(w, x2 + side_width)
#         cv2.rectangle(mask, (right_x1, left_y1), (right_x2, left_y2), 255, -1)
#
#         # 5. Dilate Ä‘á»ƒ má»Ÿ rá»™ng mask
#         kernel = np.ones((self.config.MASK_DILATE, self.config.MASK_DILATE), np.uint8)
#         mask = cv2.dilate(mask, kernel, iterations=2)
#
#         # 6. Blur Ä‘á»ƒ lÃ m má»m edges
#         mask = cv2.GaussianBlur(mask, (21, 21), 11)
#
#         # 7. Threshold vá» binary
#         _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)
#
#         # DEBUG: LÆ°u mask tÃ³c
#         debug_dir = "./debug_masks"
#         import os
#         os.makedirs(debug_dir, exist_ok=True)
#         cv2.imwrite(f'{debug_dir}/debug_hair7_mask.png', mask)
#
#         # DEBUG: Táº¡o overlay visualization
#         overlay = image.copy()
#         overlay[mask > 0] = overlay[mask > 0] * 0.5 + np.array([0, 255, 0]) * 0.5  # Xanh = tÃ³c
#
#         # Váº½ Ä‘Æ°á»ng chia
#         cv2.line(overlay, (0, hairline_y), (w, hairline_y), (0, 0, 255), 2)
#
#         # Váº½ bbox máº·t
#         x1, y1, x2, y2 = face_info['bbox']
#         cv2.rectangle(overlay, (x1, y1), (x2, y2), (255, 0, 0), 2)
#
#         cv2.imwrite(f'{debug_dir}/debug_hair_overl4ay.png', overlay)
#
#         print(f"âœ… Hair mask: {np.sum(mask==255)} white pixels ({np.sum(mask==255)/mask.size*100:.1f}%)")
#         print(f"âœ… Hairline y-position: {hairline_y}")
#
#         return mask
#
#
# class HairStyleGeneratorSDXLMediaPipe:
#     """SDXL Inpainting vá»›i MediaPipe Face Detection"""
#
#     def __init__(self, config: Optional[HairStyleConfig] = None):
#         self.config = config or HairStyleConfig()
#         self.device = self.config.DEVICE
#
#         logger.info(f"Initializing SDXL Inpainting with MediaPipe")
#         logger.info(f"Device: {self.device}")
#
#         # Khá»Ÿi táº¡o MediaPipe face detector - TRUYá»€N CONFIG VÃ€O
#         self.face_detector = FaceDetectorMediaPipe(self.config)
#         self.hair_mask_generator = HairMaskGenerator(self.config)
#         self.pipe = None
#
#         self._load_models()
#
#         logger.info("âœ“ SDXL + MediaPipe Generator initialized!")
#
#     def _load_models(self):
#         """Load SDXL Inpainting pipeline"""
#         logger.info("Loading SDXL Inpainting models...")
#
#         try:
#             logger.info(f"Loading {self.config.SDXL_INPAINT_MODEL}...")
#             self.pipe = AutoPipelineForInpainting.from_pretrained(
#                 self.config.SDXL_INPAINT_MODEL,
#                 torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
#                 variant="fp16" if self.device == "cuda" else None,
#                 use_safetensors=True,
#             )
#
#             self.pipe = self.pipe.to(self.device)
#
#             # Optimizations
#             if self.device == "cuda":
#                 if self.config.USE_XFORMERS:
#                     try:
#                         self.pipe.enable_xformers_memory_efficient_attention()
#                         logger.info("âœ“ xFormers enabled")
#                     except:
#                         logger.warning("xFormers not available")
#
#             # Scheduler
#             self.pipe.scheduler = UniPCMultistepScheduler.from_config(
#                 self.pipe.scheduler.config
#             )
#
#             logger.info("âœ“ SDXL Inpainting pipeline loaded!")
#
#         except Exception as e:
#             logger.error(f"Error loading models: {e}")
#             self._load_fallback_models()
#
#     def _load_fallback_models(self):
#         """Fallback model"""
#         try:
#             self.pipe = StableDiffusionXLInpaintPipeline.from_pretrained(
#                 "stabilityai/stable-diffusion-xl-base-1.0",
#                 torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
#                 variant="fp16" if self.device == "cuda" else None,
#                 use_safetensors=True,
#             )
#             self.pipe = self.pipe.to(self.device)
#             logger.info("âœ“ SDXL base model loaded as fallback")
#         except Exception as e:
#             logger.error(f"Fallback also failed: {e}")
#             raise
#
#     def preprocess_image_and_mask(
#         self,
#         image: np.ndarray
#     ) -> Tuple[Image.Image, Image.Image, Dict]:
#         """Preprocess áº£nh vÃ  táº¡o mask vá»›i MediaPipe"""
#         target_w, target_h = self.config.IMAGE_SIZE
#         h, w = image.shape[:2]
#
#         # Resize giá»¯ tá»‰ lá»‡
#         scale = min(target_w / w, target_h / h)
#         new_w, new_h = int(w * scale), int(h * scale)
#
#         resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)
#
#         # Pad
#         pad_w = (target_w - new_w) // 2
#         pad_h = (target_h - new_h) // 2
#
#         padded = cv2.copyMakeBorder(
#             resized,
#             pad_h, target_h - new_h - pad_h,
#             pad_w, target_w - new_w - pad_w,
#             cv2.BORDER_CONSTANT,
#             value=(255, 255, 255)
#         )
#
#         # Convert to PIL
#         rgb_image = cv2.cvtColor(padded, cv2.COLOR_BGR2RGB)
#         pil_image = Image.fromarray(rgb_image)
#
#         # 1. Face detection vá»›i MediaPipe
#         face_info = self.face_detector.get_face_info(padded)
#
#         # 2. Táº¡o masks
#         if face_info:
#             print(f"âœ… Face detected: {face_info['bbox']}")
#
#             # A. Mask báº£o vá»‡ khuÃ´n máº·t (tá»« trÃ¡n xuá»‘ng)
#             face_protection_mask = self.face_detector.create_face_protection_mask(
#                 padded, face_info
#             )
#
#             # B. Mask vÃ¹ng tÃ³c (phÃ­a trÃªn trÃ¡n)
#             hair_mask = self.hair_mask_generator.create_hair_mask(
#                 padded, face_info, self.face_detector
#             )
#
#             # 3. Káº¿t há»£p masks:
#             # Logic: Chá»‰ thay Ä‘á»•i vÃ¹ng tÃ³c (hair_mask) VÃ€ KHÃ”NG pháº£i vÃ¹ng máº·t (invert face_protection_mask)
#
#             # Invert face protection mask: máº·t = 0, khÃ¡c = 255
#             not_face_mask = face_protection_mask  # ÄÃ£ lÃ  máº·t=0, khÃ¡c=255
#
#             # Final mask = hair_mask AND not_face_mask
#             final_mask = cv2.bitwise_and(hair_mask, not_face_mask)
#
#             # DEBUG: LÆ°u táº¥t cáº£ masks
#             debug_dir = "./debug_masks"
#             import os
#             os.makedirs(debug_dir, exist_ok=True)
#             cv2.imwrite(f'{debug_dir}/debug_combined_mask5.png', final_mask)
#
#             # 4. Äáº£m báº£o vÃ¹ng máº·t hoÃ n toÃ n Ä‘en (0)
#             x1, y1, x2, y2 = face_info['bbox']
#             # Má»Ÿ rá»™ng má»™t chÃºt Ä‘á»ƒ cháº¯c cháº¯n
#             padding = 10
#             x1 = max(0, x1 - padding)
#             y1 = max(0, y1 - padding)
#             x2 = min(final_mask.shape[1], x2 + padding)
#             y2 = min(final_mask.shape[0], y2 + padding)
#             final_mask[y1:y2, x1:x2] = 0
#
#             # 5. Statistics
#             mask_stats = {
#                 'hair_pixels': int(np.sum(hair_mask == 255)),
#                 'face_protected_pixels': int(np.sum(face_protection_mask == 255)),
#                 'final_changed_pixels': int(np.sum(final_mask == 255)),
#                 'hair_percentage': f"{np.sum(hair_mask == 255)/hair_mask.size*100:.1f}%",
#                 'change_percentage': f"{np.sum(final_mask == 255)/final_mask.size*100:.1f}%",
#             }
#
#             print(f"ğŸ“Š Mask Statistics:")
#             for key, value in mask_stats.items():
#                 print(f"  {key}: {value}")
#
#         else:
#             print("âš ï¸ No face detected, using fallback mask")
#             # KhÃ´ng cÃ³ face: mask ná»­a trÃªn áº£nh
#             final_mask = np.zeros((padded.shape[0], padded.shape[1]), dtype=np.uint8)
#             cv2.rectangle(final_mask, (0, 0), (padded.shape[1], padded.shape[0] // 2), 255, -1)
#             mask_stats = {}
#
#         # Convert mask to PIL
#         pil_mask = Image.fromarray(final_mask).convert("L")
#
#         processing_info = {
#             'original_size': (w, h),
#             'resized_size': (new_w, new_h),
#             'padded_size': (target_w, target_h),
#             'has_face': face_info is not None,
#             'face_bbox': face_info['bbox'] if face_info else None,
#             'mask_stats': mask_stats if face_info else {}
#         }
#
#         return pil_image, pil_mask, processing_info
#
#     def enhance_prompt_for_inpainting(
#         self,
#         base_prompt: str,
#         face_info: Optional[Dict] = None
#     ) -> Tuple[str, str]:
#         """Enhanced prompts vá»›i nháº¥n máº¡nh báº£o vá»‡ máº·t"""
#
#         gender = "person"
#         if face_info:
#             # MediaPipe khÃ´ng cÃ³ gender detection, cÃ³ thá»ƒ thÃªm náº¿u cáº§n
#             gender = "person"
#
#         positive = f"{base_prompt}, "
#         positive += "ORIGINAL FACE PRESERVED COMPLETELY, same person identity, "
#         positive += "facial features unchanged, only hairstyle modification, "
#         positive += "hair transformation above forehead, "
#         positive += "realistic hair texture, detailed hairstyle, "
#         positive += "professional haircut, 8k uhd, sharp focus, best quality"
#
#         negative = "different face, changed face, altered face, new person, "
#         negative += "face swap, identity change, distorted face, "
#         negative += "blurry face, ugly face, bad anatomy, "
#         negative += "unnatural hair, wig, fake hair, hairpiece, "
#         negative += "low quality, artifacts, watermark, signature"
#
#         return positive, negative
#
#     def generate_single_style(
#         self,
#         image: np.ndarray,
#         style_name: str,
#         seed: Optional[int] = None,
#         num_steps: Optional[int] = None,
#         denoising_strength: Optional[float] = None,
#         use_mask: bool = True
#     ) -> Dict:
#         """Generate vá»›i MediaPipe face preservation"""
#
#         # Clear cache
#         if torch.cuda.is_available():
#             torch.cuda.empty_cache()
#
#         logger.info(f"ğŸ¯ Generating: {style_name}")
#         start_time = time.time()
#
#         # Get style config
#         from app.config.hair_config import HairStylePrompts as StyleConfig
#
#         if style_name not in StyleConfig.HAIR_STYLES:
#             raise ValueError(f"Unknown style: {style_name}")
#
#         style_config = StyleConfig.HAIR_STYLES[style_name]
#
#         # Preprocess áº£nh vÃ  mask
#         pil_image, pil_mask, processing_info = self.preprocess_image_and_mask(image)
#
#         # Enhanced prompts
#         base_prompt = style_config['prompt']
#         positive_prompt, negative_prompt = self.enhance_prompt_for_inpainting(
#             base_prompt,
#             processing_info['face_bbox']
#         )
#
#         # Seed
#         if seed is None:
#             seed = int(time.time() * 1000) % 1000000
#
#         generator = torch.Generator(device=self.device).manual_seed(seed)
#
#         # Parameters
#         num_steps = num_steps or self.config.NUM_INFERENCE_STEPS
#         denoising = denoising_strength or self.config.DENOISING_STRENGTH
#
#         # Äáº£m báº£o denoising Ä‘á»§ cao
#         if denoising < 0.6:
#             denoising = 0.7
#             logger.info(f"âš ï¸ Adjusted denoising to {denoising}")
#
#         logger.info(f"âš™ï¸ Settings: steps={num_steps}, denoising={denoising}")
#         logger.info(f"ğŸ“ Prompt: {positive_prompt[:80]}...")
#
#         # Generate
#         if use_mask:
#             output = self.pipe(
#                 prompt=positive_prompt,
#                 negative_prompt=negative_prompt,
#                 image=pil_image,
#                 mask_image=pil_mask,
#                 num_inference_steps=num_steps,
#                 guidance_scale=self.config.GUIDANCE_SCALE,
#                 strength=denoising,
#                 generator=generator,
#             )
#         else:
#             output = self.pipe(
#                 prompt=positive_prompt,
#                 negative_prompt=negative_prompt,
#                 image=pil_image,
#                 num_inference_steps=num_steps,
#                 guidance_scale=self.config.GUIDANCE_SCALE,
#                 strength=denoising,
#                 generator=generator,
#             )
#
#         result_image = output.images[0]
#
#         # OPTIONAL: Manual face restoration Ä‘á»ƒ cháº¯c cháº¯n
#         if processing_info['has_face'] and use_mask:
#             # CÃ³ thá»ƒ thÃªm logic paste láº¡i máº·t gá»‘c náº¿u cáº§n
#             pass
#
#         # Crop vá» kÃ­ch thÆ°á»›c gá»‘c
#         if processing_info.get('padded_size'):
#             result_array = np.array(result_image)
#             padded_w, padded_h = processing_info['padded_size']
#             new_w, new_h = processing_info['resized_size']
#             pad_w = (padded_w - new_w) // 2
#             pad_h = (padded_h - new_h) // 2
#
#             cropped = result_array[
#                 pad_h:pad_h + new_h,
#                 pad_w:pad_w + new_w
#             ]
#             result_image = Image.fromarray(cropped)
#
#         elapsed_time = time.time() - start_time
#         logger.info(f"âœ… Done in {elapsed_time:.2f}s")
#
#         return {
#             'result': result_image,
#             'mask': pil_mask,
#             'processing_info': processing_info,
#             'prompts': {
#                 'positive': positive_prompt,
#                 'negative': negative_prompt
#             },
#             'settings': {
#                 'seed': seed,
#                 'steps': num_steps,
#                 'denoising': denoising
#             }
#         }
#
#
# # Singleton
# _generator_instance = None
#
#
# def get_hair_generator() -> HairStyleGeneratorSDXLMediaPipe:
#     """Get generator (singleton)"""
#     global _generator_instance
#
#     if _generator_instance is None:
#         logger.info("Creating SDXL + MediaPipe Hair Generator")
#         _generator_instance = HairStyleGeneratorSDXLMediaPipe()
#
#     return _generator_instance
#
#
# def cleanup_generator():
#     """Cleanup"""
#     global _generator_instance
#
#     if _generator_instance is not None:
#         del _generator_instance
#         _generator_instance = None
#
#         if torch.cuda.is_available():
#             torch.cuda.empty_cache()
#
#
# """
# CÃ€I Äáº¶T:
# pip install mediapipe opencv-python
#
# USAGE:
# from hairstyle_service_sdxl_mediapipe import get_hair_generator
# import cv2
#
# generator = get_hair_generator()
# image = cv2.imread("input.jpg")
#
# result = generator.generate_single_style(
#     image=image,
#     style_name="man_bun",
#     denoising_strength=0.75,
#     seed=None
# )
#
# result['result'].save("output.jpg")
# result['mask'].save("mask.png")
#
# # Check debug images:
# # - debug_masks/debug_hair_ma7sk.png: Mask vÃ¹ng tÃ³c
# # - debug_masks/debug_face_protection_7mask.png: Mask báº£o vá»‡ máº·t
# # - debug_masks/debug_combined_m9ask.png: Mask cuá»‘i cÃ¹ng
# # - debug_masks/debug_hair_overla7y.png: Visualization overlay
# """